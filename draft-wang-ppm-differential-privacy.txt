



Privacy Preserving Measurement                                   S. Wang
Internet-Draft                                                Apple Inc.
Intended status: Informational                         18 September 2023
Expires: 21 March 2024


                Differential Privacy Mechanisms for DAP
               draft-wang-ppm-differential-privacy-latest

Abstract

   TODO Abstract

About This Document

   This note is to be removed before publishing as an RFC.

   The latest revision of this draft can be found at
   https://wangshan.github.io/draft-wang-ppm-differential-privacy/draft-
   wang-ppm-differential-privacy.html.  Status information for this
   document may be found at https://datatracker.ietf.org/doc/draft-wang-
   ppm-differential-privacy/.

   Discussion of this document takes place on the Privacy Preserving
   Measurement Working Group mailing list (mailto:ppm@ietf.org), which
   is archived at https://mailarchive.ietf.org/arch/browse/ppm/.
   Subscribe at https://www.ietf.org/mailman/listinfo/ppm/.

   Source for this draft and an issue tracker can be found at
   https://github.com/wangshan/draft-wang-ppm-differential-privacy.

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at https://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on 21 March 2024.

Copyright Notice

   Copyright (c) 2023 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents (https://trustee.ietf.org/
   license-info) in effect on the date of publication of this document.
   Please review these documents carefully, as they describe your rights
   and restrictions with respect to this document.  Code Components
   extracted from this document must include Revised BSD License text as
   described in Section 4.e of the Trust Legal Provisions and are
   provided without warranty as described in the Revised BSD License.

Table of Contents

   1.  Introduction
   2.  Conventions and Definitions
   3.  Security Goals and Threat Model
   4.  DP Mechanisms
     4.1.  Discrete Laplace
     4.2.  Discrete Gaussian
     4.3.  Randomized Response
   5.  DP Policies for VDAFs
     5.1.  Concrete Instantiations of DP Policies with VDAFs
       5.1.1.  Prio3SumVec, bits = 1, with Randomized Response
               Client-DP
       5.1.2.  Prio3Histogram with Discrete Gaussian
   6.  Executing DP Policies in DAP
   7.  Security Considerations
   8.  IANA Considerations
   9.  References
     9.1.  Normative References
     9.2.  Informative References
   Appendix A.  Contributors
   Appendix B.  Overview of Differential Privacy
     B.1.  Differential privacy models
     B.2.  Differential privacy levels
     B.3.  Protected entity
     B.4.  Privacy budget and accounting
     B.5.  Pure EPSILON-DP, or (EPSILON, DELTA)-approximate DP
       B.5.1.  (ALPHA, TAU)-RÃ©nyi DP
       B.5.2.  Zero Concentrated-DP
     B.6.  Sensitivity
     B.7.  Data type and Noise type
   Author's Address

1.  Introduction

   Multi-party computation systems like the Distributed Aggregration
   Protocol [DAP] enable secure aggregation of measurements generated by
   individuals without handling the measurements in the clear.  This is
   made possible by using a Verifiable Distributed Aggregation Function
   [VDAF], the core cryptographic component of DAP.  Execution of A VDAF
   involves: a large set of "Clients" who produce cryptographically
   protected measurements, called "reports"; a small number of
   "Aggregators" who consume reports and produce the cryptographically
   protected aggregate; and a "Collector" who consumes the aggregate
   result.  Distributing the computation of the aggregate in this manner
   ensures that, as long as one Aggregator is honest, no attacker can
   learn an honest Client's measurement.

   Depending on the application, protecting the measurements may not be
   sufficient for privacy, since the aggregate itself can reveal
   privacy-sensitive information.  As an illustrative example, consider
   using DAP/VDAF to summarize the distribution of the heights of
   respondents to a survey.  If one of the respondents is especially
   short or tall, then their contribution is likely to skew the summary
   statistic in a way that reveals their height.  Ideally, no individual
   measurement would have such a signficant impact on the aggregate
   result, but in general such leakage is inevitable.

   This intuition can be formalized by the notion differential privacy
   [DMNS06].  Differentially privacy is a property of an algorithm or
   protocol that computes some function of a set of measurements.  We
   say the algorithm or protocol is "differentially private", or "DP",
   if the probability of observing a particular output does not change
   signficantly as a result of removing one of the measurements (or
   substituting it with another).

   VDAFs are not DP on their own, but they can be composed with a
   variety of mechanisms that endow them with this property.  All such
   mechanisms work by introducing "noise" into the computation that is
   carefully calibrated for a number of application-specific parameters,
   including the structure and number of measurements and the desired
   aggregation function.

   Noise can be introduced at various steps at the computation, and by
   various parties.  Depending on the mechanism: the Clients might add
   noise to their own measurements; and the Aggregators might add noise
   to their aggregate shares (the values they produce for the
   Collector).

   In this document, we shall refer to the composition of DP mechanisms
   into a scheme that provides (some notion of) DP as a "DP policy".
   For some policies, noise is added only by the Clients or only by the
   Aggregators, but for others, noise might be added by both Clients and
   Aggregators.

   The primary goal of this document is to specify how DP policies are
   implemented in DAP.  It does so in the following stages:

   1.  Section 3 describes the notion(s) of DP that are compatible with
       DAP and the threat model in which we hope to achieve DP.  It also
       provides a systematization of applicable DP policies from the
       literature.

   2.  Section 4 specifies various mechanisms required for building DP
       systems, including algorithms for sampling from discrete Laplace
       and Gaussian distributions.

   3.  Section 5 defines DP policies and specifies concrete policies for
       endowing VDAFs in [VDAF] with DP.  [TODO: And other drafts, once
       they appear.]  [CP: This API may not be compatible with all DP
       polices we might want to implement.]

   4.  Section 6 specifies the integration of DP policies from the
       previous section into DAP.  In particular, it describes changes
       to the Client, Aggregator, and Collector behavior required to
       implement the policy.  [CP: This integration might not be
       compatible with all DP policies we might want to implement.]

   The following considerations are out-of-scope for this document:

   1.  Apart from privacy, the primary consideration for choosing a DP
       policy and calibrating it for an application is "utility".
       Intuitively, the more noise is added, the stronger the DP
       guarantee, but adding too much noise can reduce the veracity of
       the result.  This document provides no guidance for this
       selection process.

   2.  This document describes a particular class of narrowly-scoped DP
       policies.  Other, more sophisticated policies are possible.
       [TODO: Add citations.]

   3.  The mechanisms described in Section 4 are intended for use beyond
       DAP/VDAF.  However, this document does not describe general-
       purpose DP policies; those described in Section 5 are tailored to
       specific VDAFs or classes of VDAFs.

2.  Conventions and Definitions

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

   This document uses the same conventions for error handling as [DAP].

      TODO: add more

3.  Security Goals and Threat Model

   TODO

4.  DP Mechanisms

   This section describes various mechanisms required for implementing
   DP policies.  The algorithms are designed to securely expand a short,
   uniform random seed into a sample from a given distribution.

   For each mechanism, we expect the noise parameters are computed based
   on the DP guarantee that it is supposed to provide.

   We also expect DP mechanisms to contain the following
   functionalities:

   *  Add noise to a piece of input data (i.e. a measurement or an
      aggregate share).  Some DP mechanisms apply noise based on the
      input data, e.g. randomized response mechanism Section 4.3 flips
      the bit at each dimension, which means the noised output depends
      on the input.

   *  Sample noise with the DP mechanism.

   *  Debias the noised data.  Note that not all noise will need this
      functionality.  Some DP mechanisms will need this functionality,
      for example, randomized response mechanisms have a debiasing step
      that removes bias.

   Therefore, we define three methods for an interface DpMechanism:

  class DpMechanism:
      DataType
      DebiasedDataType

      def add_noise(self, data: DataType) -> DataType:
          """Add noise to a piece of input data. """
          pass

      def sample_noise(self, num_reps: int, dimension: int) -> DataType:
          """
          Sample noise for `num_reps` number of times,
          with `dimension`.
          """
          pass

      def debias(self,
                 agg: DataType,
                 meas_count: int) -> DebiasedDataType:
          """
          Debias the data due to the added noise.
          This doesn't apply to all noises. Some Client-DP mechanisms
          need this functionality.
          """
          pass

4.1.  Discrete Laplace

      TODO: Specify a Laplace sampler

4.2.  Discrete Gaussian

      TODO: Specify a Gaussian sampler

4.3.  Randomized Response

      TODO: Specify any mechanisms required for randomized response
      mechanisms

5.  DP Policies for VDAFs

   The section defines a generic interface for DP policies for VDAFs.

   We will define an interface DpPolicy that composes the following:

   *  An optional Client-DP mechanism that adds noise to Clients'
      measurements.

   *  An optional Aggregator-DP mechanism that adds noise to an
      Aggregator's aggregate share, based on the number of measurements,
      and the minimum batch size.

   *  An optional debiasing step that removes the bias in DP mechanisms
      (i.e.  DpMechanism.debias).

   The composition of Client- and Aggregator-DP mechanisms defines the
   DP policy for a VDAF, and enforces the DP guarantee.

  class DpPolicy:
      Measurement
      AggregateShare
      AggregateResult
      DebiasedAggregateResult

      def add_noise_to_measurement(self,
                                   meas: Measurement,
                                   ) -> Measurement:
          """
          Add noise to measurement, if required by the Client-DP
          mechanism.
          """
          pass

      def add_noise_to_agg_share(self,
                                 agg_share: AggregateShare,
                                 meas_count: Unsigned,
                                 min_batch_size: Unsigned,
                                 ) -> AggregateShare:
          """
          Add noise to aggregate share, if required by the Aggregator-DP
          mechanism.
          """
          pass

      def debias_agg_result(self,
                            agg_result: AggregateResult,
                            meas_count: Unsigned,
                            min_batch_size: Unsigned,
                            num_aggregators: Unsigned,
                            ) -> DebiasedAggregateResult:
          """
          Debias aggregate result, if any of the Client- or
          Aggregator-DP mechanism requires this operation,
          based on the number of measurements, minimum batch size,
          and the number of Aggregators.
          """
          pass

5.1.  Concrete Instantiations of DP Policies with VDAFs

5.1.1.  Prio3SumVec, bits = 1, with Randomized Response Client-DP

   Client-DP allows Clients to protect their privacy by adding noise to
   their measurements directly, as described in Appendix B.2.  Analyses
   ([FMT20] and [FMT22]) have shown that the central DP guarantee can
   also be amplified by aggregating Clients' measurements with Client-
   DP.

   For this particular instantiation of Prio3SumVec with bits = 1, i.e.
   each value in the vector is either a 0 or 1, we can apply randomized
   response Section 4.3 Client-DP to Clients' measurements, with a
   configured Client-DP with EPSILON_0 parameter.

   The DP guarantee is met, as long as there are at least "minimum batch
   size" number of Clients, each of which adds the randomized response
   Client-DP.  However, in case the minimum batch size is not met when
   Aggregators want to output their aggregate shares, each Aggregator
   can sample the same randomized response Client-DP on the "missing"
   Clients with zero-hot vectors, so the central DP guarantee can still
   be satisfied, based on the privacy amplification analysis in [FMT20]
   and [FMT22].

   class Prio3SumVecWithRandomizedResponse:
       Measurement = Vec[Unsigned]
       AggregateShare = Vec[Field]
       AggregateResult = Vec[Unsigned]
       DebiasedAggregateResult = Vec[float]

       def __init__(self,
                    rr_mechanism: DpMechanism):
           self.rr_mechanism = rr_mechanism

       def add_noise_to_measurement(self,
                                    meas: Measurement,
                                    ) -> Measurement:
           """
           Apply randomized response mechanism to measurement.
           """
           return self.rr_mechanism.add_noise(meas)

       def add_noise_to_agg_share(self,
                                  agg_share: AggregateShare,
                                  meas_count: Unsigned,
                                  min_batch_size: Unsigned,
                                  ) -> AggregateShare:
           """
           If minimum batch size is met, this function does nothing,
           because DP guarantee would have already been met.
           Otherwise, add the same randomized response noise on the
           missing Clients.
           """
           if meas_count >= batch_size:
               return agg_share
           noise = self.rr_mechanism.sample_noise(
               batch_size - meas_count, len(agg_share)
           )
           return [a + Field(b) for (a, b) in zip(agg_share, noise)]

       def debias_agg_result(self,
                             agg_result: AggregateResult,
                             meas_count: Unsigned,
                             min_batch_size: Unsigned,
                             num_aggregators: Unsigned,
                             ) -> DebiasedAggregateResult:
           """
           Debias aggregate result with randomized response.
           """
           if meas_count >= min_batch_size:
               return self.rr_mechanism.debias(agg_result, meas_count)
           # The count passed to debiasing will be:
           # the number of actual Clients + the "missing" Clients
           # sampled by all Aggregators.
           debiasing_count = \
               meas_count + \
               num_aggregators * (min_batch_size - meas_count)
           return self.rr_mechanism.debias(agg_result, debiasing_count)

      TODO(issue #10): replace rr_mechanism once we have concretely
      defined it in Section 4.3.

5.1.2.  Prio3Histogram with Discrete Gaussian

   For Prio3Histogram, we will apply an Aggregator-only DP mechanism,
   that is implemented with discrete Gaussian.

   class Prio3HistogramWithDiscreteGaussian:
       Measurement = Unsigned
       AggregateShare = Vec[Field]
       AggregateResult = Vec[int]
       DebiasedAggregateResult = AggregateResult

       def __init__(self,
                    dgauss_mechanism: DpMechanism,
                    ):
           self.dgauss_mechanism = dgauss_mechanism

       def add_noise_to_measurement(self,
                                    meas: Measurement,
                                    ) -> Measurement:
           """
           No Client-DP here.
           """
           return meas

       def add_noise_to_agg_share(self,
                                  agg_share: AggregateShare,
                                  meas_count: Unsigned,
                                  min_batch_size: Unsigned,
                                  ) -> AggregateShare:
           """
           Sample discrete Gaussian noise, and merge it with
           aggregate share.
           """
           # Sample the noise once, with length equal to the length
           # of aggregate share.
           noise_vec = self.dgauss_mechanism.sample_noise(
               1, len(agg_share)
           )
           result = []
           for i in range(len(agg_share)):
               noise = noise_vec[i]
               if noise < 0:
                   noise = Field.MODULUS + noise
               result.append(agg_share[i] + Field(noise))
           return result

       def debias_agg_result(self,
                             agg_result: AggregateResult,
                             meas_count: Unsigned,
                             min_batch_size: Unsigned,
                             num_aggregators: Unsigned,
                             ) -> DebiasedAggregateResult:
           """
           No debiasing.
           """
           return agg_result

      TODO(issue #10): replace dgauss_mechanism once we have concretely
      defined it in Section 4.2.

6.  Executing DP Policies in DAP

      TODO: Specify integration of a DpPolicy into DAP.

7.  Security Considerations

   TODO Security

8.  IANA Considerations

   This document has no IANA actions.

9.  References

9.1.  Normative References

   [DAP]      Geoghegan, T., Patton, C., Rescorla, E., and C. A. Wood,
              "Distributed Aggregation Protocol for Privacy Preserving
              Measurement", Work in Progress, Internet-Draft, draft-
              ietf-ppm-dap-05, 10 July 2023,
              <https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap-
              05>.

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://www.rfc-editor.org/rfc/rfc2119>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://www.rfc-editor.org/rfc/rfc8174>.

   [VDAF]     Barnes, R., Cook, D., Patton, C., and P. Schoppmann,
              "Verifiable Distributed Aggregation Functions", Work in
              Progress, Internet-Draft, draft-irtf-cfrg-vdaf-06, 15 June
              2023, <https://datatracker.ietf.org/doc/html/draft-irtf-
              cfrg-vdaf-06>.

9.2.  Informative References

   [BS16]     Bun, M. and T. Steinke, "Concentrated Differential
              Privacy: Simplifications, Extensions, and Lower Bounds",
              2016, <https://arxiv.org/abs/1605.02065>.

   [CKS20]    Canonne, C. L., Kamath, G., and T. Steinke, "The Discrete
              Gaussian for Differential Privacy", 2020,
              <https://arxiv.org/abs/2004.00010>.

   [DMNS06]   Dwork, C., McSherry, F., Nissim, K., and A. Smith,
              "Calibrating Noise to Sensitivity in Private Data
              Analysis", 2006,
              <https://link.springer.com/chapter/10.1007/11681878_14>.

   [DR14]     Dwork, C. and A. Roth, "The Algorithmic Foundations of
              Differential Privacy", 2014,
              <https://www.cis.upenn.edu/~aaroth/Papers/
              privacybook.pdf>.

   [FMT20]    Feldman, V., McMillan, A., and K. Talwar, "Hiding Among
              the Clones: A Simple and Nearly Optimal Analysis of
              Privacy Amplification by Shuffling", 2020,
              <https://arxiv.org/abs/2012.12803>.

   [FMT22]    Feldman, V., McMillan, A., and K. Talwar, "Stronger
              Privacy Amplification by Shuffling for RÃ©nyi and
              Approximate Differential Privacy", 2022,
              <https://arxiv.org/abs/2208.04591>.

   [KOV15]    Kairouz, P., Oh, S., and P. Viswanath, "The Composition
              Theorem for Differential Privacy", 2015,
              <http://proceedings.mlr.press/v37/kairouz15.pdf>.

   [Mir17]    Mironov, I., "RÃ©nyi Differential Privacy", 2017,
              <https://arxiv.org/abs/1702.07476>.

   [MJTBp22]  McMillan, A., Javidbakht, O., Talwar, K., Briggs, E.,
              Chatzidakis, M., Chen, J., Duchi, J., Feldman, V., Goren,
              Y., Hesse, M., Jina, V., Katti, A., Liu, A., Lyford, C.,
              Meyer, J., Palmer, A., Park, D., Park, W., Parsa, G.,
              Pelzl, P., Rishi, R., Song, C., Wang, S., and S. Zhou,
              "Private Federated Statistics in an Interactive Setting",
              2022, <https://arxiv.org/abs/2211.10082>.

Appendix A.  Contributors

   Pierre Tholoniat Columbia University pierre@cs.columbia.edu

Appendix B.  Overview of Differential Privacy

   Differential privacy is a set of techniques used to protect the
   privacy of individuals when analyzing user's data.  It provides a
   mathematical framework that ensures the analysis of a dataset does
   not reveal identifiable information about any specific individuals.
   The advantage of differential privacy is that it provides a strong,
   quantifiable and composable privacy guarantee.  The main idea of
   differential privacy is to add carefully calibrated noise to the
   results, which makes it difficult to determine with high certainty
   whether a specific individual's data was included in the results or
   not.

B.1.  Differential privacy models

   There are two model of differential privacy: deletion differential
   privacy ( deletion dp) and replacement differntial privacy
   (replacement dp).

      OPEN ISSUE: or call it substitution dp.

   For two dataset X and Y that differ only by 1 data point.  Then the
   difference between the two are: * Replacement DP: Y differs from X by
   replacing a data point in X. * Deletion DP: Y differs from X by
   removing a data point in X.

      TODO: make the definition more accurate TODO: talk about the
      relationship of the two

B.2.  Differential privacy levels

   Ther are two levels of privacy protection: local differential privacy
   (local DP) and aggregator differential privacy (aggregator DP).

      OPEN ISSUE: or call it secure aggregator dp, or central dp.

   In the local-DP settings, Clients apply noise to their own
   measurements.  In this way, Clients have some control to protect the
   privacy of their own data.  Any measurement uploaded by a Client will
   be have local dp, Client's privacy is protected even if none of the
   Aggregators is honest (although this protection may be weak).
   Furthermore, one can analyze the aggregator DP guarantee with privacy
   amplification by aggregation, assuming each Client has added the
   required amount of local DP noise, and there are at least minimum
   batch size number of Clients in the aggregation.

   In Aggregator DP settings, an Aggregator applies noise on the
   aggregation.  Aggregator DP relies on the server being secure and
   trustworthy.  Aggregators built using DAP protocol is ideal for this
   setting because DAP ensures no server can access any individual data,
   but only the aggregation.

   If there are no local DP added from client, noise added to the
   aggregation provides the privacy guarantee of the aggregation.

   One can use the Aggregator DP noise together with local DP noise to
   achieve privacy guarantee.  If the DP guarantee is achieved with a
   minimum batch size number of Clients adding local DP noise, and
   minimum batch size is not reached when a data collection task
   expires, each Aggregator can add the remaining noise by generating
   the same local DP noise, on the missing number of Clients being the
   gap between actual number of Clients and minimum batch size.

B.3.  Protected entity

      TODO: Chris P to fill: user or report, given time

B.4.  Privacy budget and accounting

   There are various types of DP guarantees and budgets that can be
   enforced.  Many applications need to query the Client data multiple
   times, for example:

   *  Federated machine learning applications require multiple
      aggregates to be computed over the same underlying data, but with
      different machine learning model parameters.

   *  [MJTBp22] describes an interactive approach of building histograms
      over multiple iterations, and Section 4.3 describes a way to track
      Client-side budget when the Client data is queried multiple times.

      TODO: have citations for machine learning

   Itâs hard for Aggregator(s) to keep track of the privacy budget over
   time, because different Clients can participate in different data
   collection tasks, and only Clients know when their data is queried.
   Therefore, Clients must enforce the privacy budget.

   There could be multiple ways to compose DP guarantees, based on
   different DP composition theorems.  In the various example DP
   guarantees below, we describe the following:

   *  A formal definition of the DP guarantee.

   *  Composition theorems that apply to the DP guarantee.

B.5.  Pure EPSILON-DP, or (EPSILON, DELTA)-approximate DP

   Pure EPSILON-DP was first proposed in [DMNS06], and a formal
   definition of (EPSILON, DELTA)-DP can be found in Definition 2.4 of
   [DR14].

   The EPSILON parameter quantifies the "privacy loss" of observing the
   outcomes of querying two databases differing by one element.  The
   smaller EPSILON is, the stronger the privacy guarantee is, that is,
   the outcomes of querying two adjacent databases are more or less the
   same.  The DELTA parameter provides a small probability of the
   privacy loss exceeding EPSILON.

   One can compose multiple (EPSILON, DELTA)-approximate DP guarantees,
   per Theorem 3.4 of [KOV15].  One can also compose the guarantees in
   other types of guarantee first, such as RÃ©nyi DP Appendix B.5.1, and
   then convert the composed guarantee to approximate DP guarantee.

B.5.1.  (ALPHA, TAU)-RÃ©nyi DP

   A formal definition of RÃ©nyi DP can be found in Definitions 3 and 4
   of [Mir17].

   The intuition behind RÃ©nyi-DP is to use TAU parameter to measure the
   divergence of probability distributions of querying two adjacent
   databases, given RÃ©nyi order parameter ALPHA.  The smaller the TAU
   parameter, the harder it is to distinguish the outputs from querying
   two adjacent databases, and thus the stronger the privacy guarantee
   is.

   One can compose multiple RÃ©nyi DP guarantees based on Proposition 1
   of [Mir17].  After composition, one can convert the (ALPHA, TAU)-
   RÃ©nyi DP guarantee to (EPSILON, DELTA)-approximate DP, per
   Proposition 12 of [CKS20].

B.5.2.  Zero Concentrated-DP

   A formal definition of zero Concentrated-DP can be found in
   Definition 1.1 of [BS16].

   Zero Concentrated-DP uses different parameters from RÃ©nyi-DP, but
   uses a similar idea to measure the output distribution divergence of
   querying two adjacent databases.

   One can compose multiple zCDP guarantees, per Lemma 1.7 of [BS16].

B.6.  Sensitivity

      TODO: Chris P to fill: sensitivity, l1 vs l2

B.7.  Data type and Noise type

   Differential Privacy guarantee can only be achieved if data type is
   applied with the correct noise type.

      TODO: Junye to fill, mention DAP is expected to ensure the right
      pair of VDAF and DP mechanism

      TODO: Chris P: we will mention Prio3SumVec because that's what we
      use to describe aggregator DP with amplification

Author's Address

   Shan Wang
   Apple Inc.
   Email: shan_wang@apple.com
