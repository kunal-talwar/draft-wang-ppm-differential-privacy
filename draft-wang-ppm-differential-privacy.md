---
title: "Differential Privacy Mechanisms for DAP"
abbrev: "DP-PPM"
category: info

docname: draft-wang-ppm-differential-privacy-latest
submissiontype: IETF  # also: "independent", "IAB", or "IRTF"
number:
date:
consensus: true
v: 3
area: "Security"
workgroup: "Privacy Preserving Measurement"
keyword:
 - next generation
 - unicorn
 - sparkling distributed ledger
venue:
  group: "Privacy Preserving Measurement"
  type: "Working Group"
  mail: "ppm@ietf.org"
  arch: "https://mailarchive.ietf.org/arch/browse/ppm/"
  github: "wangshan/draft-wang-ppm-differential-privacy"
  latest: "https://wangshan.github.io/draft-wang-ppm-differential-privacy/draft-wang-ppm-differential-privacy.html"

author:
 -
    fullname: Junye Chen
    organization: Apple Inc.
    email: "junyec@apple.com"

    fullname: Christopher Patton
    organization: Cloudflare
    email: "chrispatton+ietf@gmail.com"

    fullname: Shan Wang
    organization: Apple Inc.
    email: "shan_wang@apple.com"


normative:

informative:

  FMT20:
    title: "Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling"
    author:
      - ins: V. Feldman
      - ins: A. McMillan
      - ins: K. Talwar
    date: 2020
    target: https://arxiv.org/abs/2012.12803

  FMT22:
    title: "Stronger Privacy Amplification by Shuffling for Rényi and Approximate Differential Privacy"
    author:
      - ins: V. Feldman
      - ins: A. McMillan
      - ins: K. Talwar
    date: 2022
    target: https://arxiv.org/abs/2208.04591

  MJTBp22:
    title: "Private Federated Statistics in an Interactive Setting"
    author:
      - ins: A. McMillan
      - ins: O. Javidbakht
      - ins: K. Talwar
      - ins: E. Briggs
      - ins: M. Chatzidakis
      - ins: J. Chen
      - ins: J. Duchi
      - ins: V. Feldman
      - ins: Y. Goren
      - ins: M. Hesse
      - ins: V. Jina
      - ins: A. Katti
      - ins: A. Liu
      - ins: C. Lyford
      - ins: J. Meyer
      - ins: A. Palmer
      - ins: D. Park
      - ins: W. Park
      - ins: G. Parsa
      - ins: P. Pelzl
      - ins: R. Rishi
      - ins: C. Song
      - ins: S. Wang
      - ins: S. Zhou
    date: 2022
    target: https://arxiv.org/abs/2211.10082

  Mir17:
    title: "Rényi Differential Privacy"
    author:
      - ins: I. Mironov
    date: 2017
    target: https://arxiv.org/abs/1702.07476

  CKS20:
    title: "The Discrete Gaussian for Differential Privacy"
    author:
      - ins: C. L. Canonne
      - ins: G. Kamath
      - ins: T. Steinke
    date: 2020
    target: https://arxiv.org/abs/2004.00010

  KOV15:
    title: "The Composition Theorem for Differential Privacy"
    author:
      - ins: P. Kairouz
      - ins: S. Oh
      - ins: P. Viswanath
    date: 2015
    target: http://proceedings.mlr.press/v37/kairouz15.pdf

  DMNS06:
    title: "Calibrating Noise to Sensitivity in Private Data Analysis"
    author:
      - ins: C. Dwork
      - ins: F. McSherry
      - ins: K. Nissim
      - ins: A. Smith
    date: 2006
    target: https://link.springer.com/chapter/10.1007/11681878_14

  DR14:
    title: "The Algorithmic Foundations of Differential Privacy"
    author:
      - ins: C. Dwork
      - ins: A. Roth
    date: 2014
    target: https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf

  BS16:
    title: "Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds"
    author:
      - ins: M. Bun
      - ins: T. Steinke
    date: 2016
    target: https://arxiv.org/abs/1605.02065


--- abstract

TODO Abstract


--- middle

# Introduction

Multi-party computation systems like the Distributed Aggregration Protocol
{{!DAP=I-D.draft-ietf-ppm-dap-05}} enable secure aggregation of measurements
generated by individuals without handling the measurements in the clear. This
is made possible by using a Verifiable Distributed Aggregation Function
{{!VDAF=I-D.draft-irtf-cfrg-vdaf-06}}, the core cryptographic component of DAP.
Execution of A VDAF involves: a large set of "Clients" who produce
cryptographically protected measurements, called "reports"; a small number of
"Aggregators" who consume reports and produce the cryptographically protected
aggregate; and a "Collector" who consumes the aggregate result. Distributing
the computation of the aggregate in this manner ensures that, as long as one
Aggregator is honest, no attacker can learn an honest Client's measurement.

Depending on the application, protecting the measurements may not be sufficient
for privacy, since the aggregate itself can reveal privacy-sensitive
information. As an illustrative example, consider using DAP/VDAF to summarize
the distribution of the heights of respondents to a survey. If one of the
respondents is especially short or tall, then their contribution is likely to
skew the summary statistic in a way that reveals their height. Ideally, no
individual measurement would have such a signficant impact on the aggregate
result, but in general such leakage is inevitable.

This intuition can be formalized by the notion differential privacy {{DMNS06}}.
Differentially privacy is a property of an algorithm or protocol that computes
some function of a set of measurements. We say the algorithm or protocol is
"differentially private", or "DP", if the probability of observing a particular
output does not change signficantly as a result of removing one of the
measurements (or substituting it with another).

VDAFs are not DP on their own, but they can be composed with a variety of
mechanisms that endow them with this property. All such mechanisms work by
introducing "noise" into the computation that is carefully calibrated for a
number of application-specific parameters, including the structure and number
of measurements and the desired aggregation function. [TODO: Anything else?]

Noise can be introduced at various steps at the computation, and by various
parties. Depending on the mechanism: the Clients might add noise to their own
measurements; the Aggregators might add noise to their aggregate shares (the
values they produce for the Collector); and the Collector might add noise to
the aggregate result before publishing it. We refer to these classes of
mechanisms as "Client-DP", "Aggregator-DP", and "Collector-DP" respectively.
[TODO: I put "Collector-DP" here as a strawman. Would we actually want to use
it?]

In addition, it is possible to compuse different DP mechanisms. [TODO: Say why
we would want to do this and give an illustrative example of combining
Client-DP and Aggregator-DP.]

[TODO: Transition text] This document defines a "DP policy" as the composition
of an optional Client-DP mechanism and an optional Aggregator-DP mechanism
with a VDAF.

The primary goal of this document is to specify how DP policies are implemented
in DAP. It does so in the following stages:

1. {{overview}} provides an overview of DP properties in the literature that
   apply to VDAFs in general and DAP/VDAF in particular. Many refinements of
   the basic DP notion are possible, some of which are not possible given the
   constraints imposed by DAP. This section enumerates these constraints and
   describes the baseline requirements for a DP mechanism used with DAP.

1. {{primitives}} specifies various primitives required for implementing DP
   mechansims, including algorithms for sampling from discrete Laplace and
   Gaussian distributions.

1. {{mechanisms}} defines an interface for DP mechanisms that are compatible
   with VDAFs and, for all VDAFs specified in {{!VDAF}} [TODO: And other
   drafts, once they appear], specifies concrete Client- and Aggregator-DP
   mechanisms.

1. {{policy}} specifies the integration of DP mechanisms from the previous
   section into DAP. In particular, it describes how a DP policy is implemented
   by the Client, Aggregator, and Collector alongside the execution of the
   VDAF.

The following considerations are out-of-scope for this document:

1. Apart from privacy, the primary consideration for choosing a DP policy and
   calibrating it for an application is "utility". Intuitively, the more noise
   is added, the stronger the DP guarantee, but adding too much noise can
   reduce the veracity of the result. This document provides no guidance for
   this selection process.

1. This document describes a particular composition of narrowly-scoped DP
   mechanisms. Other compositions and more sophisticated DP mechanisms are
   possible.

1. The primitives described in {{primitives}} are intended for use beyond
   DAP/VDAF. However, this document does not describe general-purpose DP
   mechanisms. The mechanisms described in {{mechanisms}} are tailored to
   specific VDAFs.

# Conventions and Definitions

{::boilerplate bcp14-tagged}

This document uses the same conventions for error handling as {{!DAP}}.

> TODO: add more


# Overview of Differential Privacy {#overview}

Differential privacy is a set of techniques used to protect the privacy of
individuals when analyzing user's data. It provides a mathematical framework
that ensures the analysis of a dataset does not reveal identifiable information
about any specific individuals. The advantage of differential privacy is that it
provides a strong, quantifiable and composable privacy guarantee. The main idea
of differential privacy is to add carefully calibrated noise to the results,
which makes it difficult to determine with high certainty whether a specific
individual's data was included in the results or not.

## Differential privacy models

There are two model of differential privacy: deletion differential privacy (
deletion dp) and replacement differntial privacy (replacement dp).

> OPEN ISSUE: or call it substitution dp.

For two dataset X and Y that differ only by 1 data point. Then the difference
between the two are:
* Replacement DP: Y differs from X by replacing a data point in X.
* Deletion DP: Y differs from X by removing a data point in X.

> TODO: make the definition more accurate
> TODO: talk about the relationship of the two


## Differential privacy levels

Ther are two levels of privacy protection: local differential privacy (local DP)
and aggregator differential privacy (aggregator DP).

> OPEN ISSUE: or call it secure aggregator dp, or central dp.

In the local-DP settings, Clients apply noise to their own measurements. In
this way, Clients have some control to protect the privacy of their own data.
Any measurement uploaded by a Client will be have local dp, Client's privacy is
protected even if none of the Aggregators is honest (although this protection
may be weak). Furthermore, one can analyze the aggregator DP guarantee with
privacy amplification by aggregation, assuming each Client has added the
required amount of local DP noise, and there are at least minimum batch size
number of Clients in the aggregation.

In Aggregator DP settings, an Aggregator applies noise on the aggregation.
Aggregator DP relies on the server being secure and trustworthy. Aggregators
built using DAP protocol is ideal for this setting because DAP ensures no server
can access any individual data, but only the aggregation.

If there are no local DP added from client, noise added to the aggregation
provides the privacy guarantee of the aggregation.

One can use the Aggregator DP noise together with local DP noise to achieve
privacy guarantee. If the DP guarantee is achieved with a minimum batch size
number of Clients adding local DP noise, and minimum batch size is not reached
when a data collection task expires, each Aggregator can add the remaining noise
by generating the same local DP noise, on the missing number of Clients being
the gap between actual number of Clients and minimum batch size.


## Protected entity

> TODO: Chris P to fill: user or report, given time

## Privacy budget and accounting {#budget}

There are various types of DP guarantees and budgets that can be enforced.
Many applications need to query the Client data multiple times, for example:

* Federated machine learning applications require multiple aggregates to be
  computed over the same underlying data, but with different machine learning
  model parameters.

* {{MJTBp22}} describes an interactive approach of building histograms over
  multiple iterations, and Section 4.3 describes a way to track Client-side
  budget when the Client data is queried multiple times.

> TODO: have citations for machine learning

It’s hard for Aggregator(s) to keep track of the privacy budget over time,
because different Clients can participate in different data collection tasks,
and only Clients know when their data is queried. Therefore, Clients must
enforce the privacy budget.

There could be multiple ways to compose DP guarantees, based on different
DP composition theorems. In the various example DP guarantees below,
we describe the following:

* A formal definition of the DP guarantee.

* Composition theorems that apply to the DP guarantee.

### Pure `EPSILON`-DP, or `(EPSILON, DELTA)`-approximate DP {#adp}

Pure `EPSILON`-DP was first proposed in {{DMNS06}}, and a formal definition of
`(EPSILON, DELTA)`-DP can be found in Definition 2.4 of {{DR14}}.

The `EPSILON` parameter quantifies the "privacy loss" of observing the outcomes
of querying two databases differing by one element. The smaller `EPSILON` is,
the stronger the privacy guarantee is, that is, the outcomes of querying two
adjacent databases are more or less the same.
The `DELTA` parameter provides a small probability of the privacy loss
exceeding `EPSILON`.

One can compose multiple `(EPSILON, DELTA)`-approximate DP guarantees, per
Theorem 3.4 of {{KOV15}}.
One can also compose the guarantees in other types of guarantee first, such as
Rényi DP {{rdp}}, and then convert the composed guarantee to approximate
DP guarantee.

### `(ALPHA, TAU)`-Rényi DP {#rdp}

A formal definition of Rényi DP can be found in Definitions 3 and 4 of
{{Mir17}}.

The intuition behind Rényi-DP is to use `TAU` parameter to measure the
divergence of probability distributions of querying two adjacent databases,
given Rényi order parameter `ALPHA`. The smaller the `TAU` parameter,
the harder it is to distinguish the outputs from querying two adjacent
databases, and thus the stronger the privacy guarantee is.

One can compose multiple Rényi DP guarantees based on Proposition 1 of
{{Mir17}}.
After composition, one can convert the `(ALPHA, TAU)`-Rényi DP guarantee to
`(EPSILON, DELTA)`-approximate DP, per Proposition 12 of {{CKS20}}.

### Zero Concentrated-DP {#zcdp}

A formal definition of zero Concentrated-DP can be found in Definition 1.1
of {{BS16}}.

Zero Concentrated-DP uses different parameters from Rényi-DP, but uses a similar
idea to measure the output distribution divergence of querying two adjacent
databases.

One can compose multiple zCDP guarantees, per Lemma 1.7 of {{BS16}}.

## Sentitity

> TODO: Chris P to fill: sensitivity, l1 vs l2

## Data type and Noise type

Differential Privacy guarantee can only be achieved if data type is applied
with the correct noise type.

> TODO: Junye to fill, mention DAP is expected to ensure the right pair of VDAF and DP mechanism

> TODO: Chris P: we will mention Prio3SumVec because that's what we use to describe aggregator DP with amplification

# Primitives for Implementing DP Mechanisms {#primitives}

This section describes various primitives required for implementing DP
mechansims. The algorithms are designed to securely expand a short, uniform
random seed into a sample from a given distribution.

## Discrete Laplace

> TODO: Specify a Laplace sampler

## Discrete Gaussian

> TODO: Specify a Gaussian sampler

## Randomized Response

> TODO: Specify any primitives required for randomized response mechanisms

# DP Mechanisms for VDAFs {#mechanisms}

The section defines a generic interface for DP mechanisms VDAFs.

> TODO: Junye to spell out the interface of dp mechanisms, "Noise Mechanisms to Accomplish DP"

## Client-DP for Prio3Histogram

> TODO: Concretely describe a Client-DP mechanism for Prio3Histogram.

## Aggregator-DP for Prio3Histogram

> TODO: Concretely describe an Aggregator-DP mechanism for Prio3Histogram.

# DP Policies for DAP {#policy}

> TODO: Junye to fill in, expand on aggregator DP, aggregator DP amplified by
local DP, includes contents in "Execution of an Aggregation Protocol with DP"


# Security Considerations

TODO Security


# IANA Considerations

This document has no IANA actions.


--- back

# Contributors
{:numbered="false"}

Pierre Tholoniat
Columbia University
pierre@cs.columbia.edu
